{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# the bare minimum viable product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import gradio as gr\n",
    "\n",
    "from onnxruntime import InferenceSession\n",
    "from PIL import Image\n",
    "\n",
    "from transformers import (\n",
    "    pipeline,\n",
    "    AutoFeatureExtractor, \n",
    "    DefaultDataCollator, \n",
    "    AutoModelForImageClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image classifier\n",
    "food_model_extractor = AutoFeatureExtractor.from_pretrained(pretrained_model_name_or_path = 'stochastic/102722run') #this can also be a tokenizer\n",
    "food_classification_model = AutoModelForImageClassification.from_pretrained(\"stochastic/102722run\")\n",
    "\n",
    "# image describer\n",
    "flan_pipe = pipeline(\"text2text-generation\", model='google/flan-t5-large',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper \n",
    "def clean_up_answer(flan_answer):\n",
    "    \n",
    "    # replace extra periods \n",
    "    if flan_answer[-1] == '.':\n",
    "        pass\n",
    "    else:\n",
    "        flan_answer = flan_answer + \".\"\n",
    "\n",
    "    # get rid of underscores from answers\n",
    "    clean_answer = flan_answer.replace(\"_\", \" \")\n",
    "\n",
    "    return clean_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7866\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7866/\" width=\"900\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(<gradio.routes.App at 0x2b048cb29d0>, 'http://127.0.0.1:7866/', None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForImageClassification.from_pretrained('stochastic/102722run')\n",
    "extractor =  AutoFeatureExtractor.from_pretrained(pretrained_model_name_or_path = 'stochastic/102722run')\n",
    "\n",
    "def classify_and_describe_image(user_input):\n",
    "    \"\"\"\n",
    "    Take an image and describe how to eat it\n",
    "    \"\"\"\n",
    "\n",
    "    session = InferenceSession(\"../../onnx/vit-model.onnx\")\n",
    "    output_name = session.get_outputs()[0].name\n",
    "\n",
    "    inputs = food_model_extractor(Image.open(user_input).convert(\"RGB\"), return_tensors=\"np\") #onnx expects numpy\n",
    "    outputs = session.run(output_names = [output_name], input_feed=dict(inputs))\n",
    "\n",
    "    predicted_class_idx = outputs[0].argmax(-1).item()\n",
    "    predicted_food = food_classification_model.config.id2label[predicted_class_idx]\n",
    "    first_sentence = f\"This looks like {predicted_food}! \"\n",
    "\n",
    "    prompts = {\n",
    "        f\"Answer the following question: How do you enjoy {predicted_food}?\"                       : f'I recommend to ',\n",
    "        f\"Answer the following question: What are the flavors of {predicted_food}\"                 : 'The flavors are similar to ',\n",
    "        f\"Answer the following question: What are the textures of {predicted_food}?\"               : 'The textures are ',\n",
    "        f\"Answer the following question: What are the aromas of {predicted_food}?\"                 : 'The armoas will be ',\n",
    "        f\"Answer the following question: How do you improve the flavor of {predicted_food}?\"       : 'To enjoy, you can '\n",
    "    }\n",
    "\n",
    "    answers = []\n",
    "    for question in prompts.keys():\n",
    "        answer = flan_pipe(question, max_length = 100)[0][\"generated_text\"].lower()\n",
    "        if answer[-1] == '.':\n",
    "          pass\n",
    "        else:\n",
    "          answer = answer + \".\"\n",
    "        rec = prompts[question] + answer\n",
    "        answers.append(rec)\n",
    "    return clean_up_answer(first_sentence + \" \".join(answers))\n",
    "    \n",
    "\n",
    "gr.Interface(fn=classify_and_describe_image, \n",
    "             inputs=gr.Image(type = 'filepath', label = \"Image\"),\n",
    "             outputs=[gr.Textbox(lines=3, label=f\"Here are the ways you can enjoy this and improve it!\")],\n",
    "             title = \"Upload food you haven't tried before\",\n",
    ").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## todo\n",
    "\n",
    "- try out banana\n",
    "- Add a flagger that saves what part of the recommendation is bad so the model can continually learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mvp to banana and load testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "import base64\n",
    "import mimetypes\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "inference server error: taskID does not exist: task_36331a05-5204-47fc-90a7-4a4b23a237c2. This is a general inference pipeline error, and could be due to: \n-- An inference runtime error crashed the server before it could return 500 --> be sure to test for runtime crashes on your own GPU.\n-- The payload in or out was too large --> current limit is 50mb.\n-- The model was not yet fully deployed --> try again later once the dashboard confirms deployed.\n-- (Rare) Banana's GPUs are at capacity and the scaleup timed out --> try again later.\n\t\t\t The Banana infra team is working hard to resolve these.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\truon\\Documents\\projects\\food\\notebooks\\combined_models\\mvp.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/truon/Documents/projects/food/notebooks/combined_models/mvp.ipynb#X15sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m model_key \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mfefd28eb-0a7f-49c2-967c-d878d5412e6d\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/truon/Documents/projects/food/notebooks/combined_models/mvp.ipynb#X15sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m model_inputs \u001b[39m=\u001b[39m {\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/truon/Documents/projects/food/notebooks/combined_models/mvp.ipynb#X15sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mprompt\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mHello World! I am a [MASK] machine learning model.\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/truon/Documents/projects/food/notebooks/combined_models/mvp.ipynb#X15sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m }\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/truon/Documents/projects/food/notebooks/combined_models/mvp.ipynb#X15sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m out \u001b[39m=\u001b[39m banana\u001b[39m.\u001b[39;49mrun(api_key, model_key, model_inputs)\n",
      "File \u001b[1;32mc:\\Users\\truon\\anaconda3\\envs\\learn\\lib\\site-packages\\banana_dev\\package.py:5\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(api_key, model_key, model_inputs)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun\u001b[39m(api_key, model_key, model_inputs):\n\u001b[1;32m----> 5\u001b[0m     out \u001b[39m=\u001b[39m run_main(\n\u001b[0;32m      6\u001b[0m         api_key \u001b[39m=\u001b[39;49m api_key, \n\u001b[0;32m      7\u001b[0m         model_key \u001b[39m=\u001b[39;49m model_key, \n\u001b[0;32m      8\u001b[0m         model_inputs \u001b[39m=\u001b[39;49m model_inputs\n\u001b[0;32m      9\u001b[0m     )\n\u001b[0;32m     10\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\truon\\anaconda3\\envs\\learn\\lib\\site-packages\\banana_dev\\generics.py:37\u001b[0m, in \u001b[0;36mrun_main\u001b[1;34m(api_key, model_key, model_inputs)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[39m# else it's long running, so poll for result\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m---> 37\u001b[0m     dict_out \u001b[39m=\u001b[39m check_api(api_key, result[\u001b[39m\"\u001b[39;49m\u001b[39mcallID\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[0;32m     38\u001b[0m     \u001b[39mif\u001b[39;00m dict_out[\u001b[39m'\u001b[39m\u001b[39mmessage\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mlower() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msuccess\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m     39\u001b[0m         \u001b[39mreturn\u001b[39;00m dict_out\n",
      "File \u001b[1;32mc:\\Users\\truon\\anaconda3\\envs\\learn\\lib\\site-packages\\banana_dev\\generics.py:112\u001b[0m, in \u001b[0;36mcheck_api\u001b[1;34m(api_key, call_id)\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n\u001b[0;32m    111\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m--> 112\u001b[0m     \u001b[39mraise\u001b[39;00m e\n",
      "File \u001b[1;32mc:\\Users\\truon\\anaconda3\\envs\\learn\\lib\\site-packages\\banana_dev\\generics.py:109\u001b[0m, in \u001b[0;36mcheck_api\u001b[1;34m(api_key, call_id)\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    108\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m out[\u001b[39m'\u001b[39m\u001b[39mmessage\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mlower():\n\u001b[1;32m--> 109\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(out[\u001b[39m'\u001b[39m\u001b[39mmessage\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m    110\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n\u001b[0;32m    111\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[1;31mException\u001b[0m: inference server error: taskID does not exist: task_36331a05-5204-47fc-90a7-4a4b23a237c2. This is a general inference pipeline error, and could be due to: \n-- An inference runtime error crashed the server before it could return 500 --> be sure to test for runtime crashes on your own GPU.\n-- The payload in or out was too large --> current limit is 50mb.\n-- The model was not yet fully deployed --> try again later once the dashboard confirms deployed.\n-- (Rare) Banana's GPUs are at capacity and the scaleup timed out --> try again later.\n\t\t\t The Banana infra team is working hard to resolve these."
     ]
    }
   ],
   "source": [
    "import banana_dev as banana\n",
    "\n",
    "api_key = \"\"\n",
    "model_key = \"\"\n",
    "\n",
    "model_inputs = {\n",
    "    'prompt': 'Hello World! I am a [MASK] machine learning model.'\n",
    "}\n",
    "out = banana.run(api_key, model_key, model_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_file_name = \"../../data/food-101/images/churros/1061830.jpg\"\n",
    "def generate_image_prediction():\n",
    "    with open(image_file_name, \"rb\") as image_file:\n",
    "        image_bytes = BytesIO(image_file.read())\n",
    "\n",
    "    print(\n",
    "        banana.run(\n",
    "            api_key,\n",
    "            model_key,\n",
    "            {\n",
    "                \"image\": base64.b64encode(image_bytes.getvalue()).decode(\"utf-8\"),\n",
    "                \"filename\": os.path.basename(image_file_name),\n",
    "            },\n",
    "        )\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('learn')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6efa6cdb99600126c800a8c2796f4efb4f3deae8ebc43a754c2171b5bde04e09"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
